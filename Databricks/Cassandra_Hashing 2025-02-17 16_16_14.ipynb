{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8665c5bb-5b39-4c33-87af-a9cd677de485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "''' HASHING in CASSANDRA\n",
    "\n",
    "------------------\n",
    "Classroom Notes\n",
    "------------------\n",
    "•\tRecall that Cassandra organizes the nodes of a cluster into a logical ring. As a key-value store, Cassandra can look up or store data, locating where it physically lives in the cluster, based on the hash of the data’s key.  \n",
    "•\tWe compute the hash of a key by passing the key into a hash function. A hash function is any function that can map data objects of arbitrary size down into values of fixed-size. For example, we might maps strings, which in principle can be any finite length, down to 64-bit integers, which are 8 bytes in size. You can find a discussion of hashing and hash functions in any introductory data structures and algorithms textbook.\n",
    "\n",
    "------------------\n",
    " Other Notes \n",
    "------------------\n",
    "1. Introduction to Hashing in Cassandra\n",
    "   - Apache Cassandra is a distributed NoSQL database designed for high availability.\n",
    "   - It uses **consistent hashing** to distribute data across nodes in a cluster.\n",
    "\n",
    "2. Partitioning in Cassandra\n",
    "   - Cassandra partitions data using a **partition key**.\n",
    "   - The partition key is hashed using the **Murmur3 hashing function**.\n",
    "   - The hash value determines which **virtual node (vnode)** stores the data.\n",
    "\n",
    "3. Murmur3 Hashing\n",
    "   - Cassandra uses Murmur3 as its default hashing function.\n",
    "   - Murmur3 is a non-cryptographic, fast hash function optimized for hash-based lookups.\n",
    "\n",
    "4. How Hashing Works in Cassandra\n",
    "   - Given a **partition key**, Cassandra applies Murmur3 hash:\n",
    "       ```\n",
    "       token = Murmur3(partition_key)\n",
    "       ```\n",
    "   - The hash value (token) is mapped to a node in the **token ring**.\n",
    "   - The node responsible for that token stores the data.\n",
    "\n",
    "5. Consistent Hashing & Token Ring\n",
    "   - Cassandra nodes are arranged in a **token ring**.\n",
    "   - Each node owns a range of hash values (tokens).\n",
    "   - When a new node joins, it takes over part of the token space, ensuring balanced distribution.\n",
    "   - If a node fails, adjacent nodes in the ring take over its tokens.\n",
    "\n",
    "6. Replication & Hashing\n",
    "   - Cassandra replicates data across multiple nodes using **Replication Factor (RF)**.\n",
    "   - The replication strategy defines how replicas are placed:\n",
    "       - **SimpleStrategy**: Replicas are placed on consecutive nodes in the ring.\n",
    "       - **NetworkTopologyStrategy**: Replicas are placed across data centers.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d0f97a-9564-4c23-b1b3-c7a515ebb8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import scala.util.hashing.MurmurHash3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f31043-6ce0-46e4-a4d8-8f61aa5bcbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val s = \"The Ballad of Wendell Scott\"\n",
    "val h = MurmurHash3.stringHash(s)\n",
    "//This should give me a hash vaue of -761502732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1d77d4-f59a-499c-84aa-1ac9f58a230f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Imagine you have a Cassandra cluster made of eight nodes, whose names, which we will take as the unique ID of each machine are: { node0, node1, …, node7 }. \n",
    "\n",
    "//•\tIn your notebook, create a collection of eight node names according to the format above, i.e. node0, node1,…,node7.\n",
    "//•\tIt’s possible to create the collection of names with a concise one-liner.  Indeed, you might want to try to create the names in such a manner to practice writing succinct Scala code, rather than  brute forcing things by explicitly typing out all the node names by hand.\n",
    "\n",
    "// Creating them manually:\n",
    "val nodes = Seq(\"node0\", \"node1\", \"node2\", \"node3\", \"node4\", \"node5\", \"node6\", \"node7\")\n",
    "println(nodes)\n",
    "\n",
    "val nodeslist = (0 to 7).map(i => s\"node$i\").toSeq //list vs. vector\n",
    "println(nodeslist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931e81b0-23c7-4050-ace6-0580441c03e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Computing the MurmurHash values for each of the node names and sort the results in ascending order by their MurmurHash values.\n",
    "val nodeHashes = nodeslist.map(node => (node, MurmurHash3.stringHash(node)))\n",
    "                          .sortBy(_._2)\n",
    "println(nodeHashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee15771-85c5-48aa-a72b-d61fa7d1ab5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "'''\n",
    "------------------\n",
    "Logical Ring Positioning\n",
    "------------------\n",
    "--- Why is the Range [-2,147,483,648 to 2,147,483,647]?\n",
    "The range of hash values comes from the fact that Cassandra (and many hashing systems) use signed 32-bit integers for their MurmurHash3 hashing function. Let's break it down:\n",
    "\n",
    "-- Why Does MurmurHash3 Output 32-bit Signed Integers?\n",
    "Cassandra uses MurmurHash3, which is a fast, non-cryptographic hash function that outputs values as signed 32-bit integers by default.\n",
    "\n",
    "When hashing a partition key (MurmurHash3.stringHash(\"some_key\")), the result is always within the signed 32-bit range.\n",
    "\n",
    "-- Why Do We Map These Values to a 360° Ring?\n",
    "Cassandra and similar distributed systems use consistent hashing, which places nodes on a circular hash space.\n",
    "\n",
    "Since hash values fall in [-2,147,483,648 to 2,147,483,647], we normalize them into a 0-360° scale.\n",
    "This helps us visualize their distribution and assign partition keys to nodes efficiently.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92481fe8-699b-4e57-a371-bf3eeb70e91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "//Imagine a user tries to insert a record whose key is “Transylvanian Christmas”. \n",
    "//What is the hash value associated with this key? \n",
    "//Which node (give the node’s name and Token ID) should the insertion of “Transylvanian Christmas” be destined for4?\n",
    "val t = \"Transylvanian Christmas\"\n",
    "val w = MurmurHash3.stringHash(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54e7735-9529-46b4-ad68-b988f6ee2987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "'''\n",
    "------------------\n",
    "Replication\n",
    "------------------\n",
    "\n",
    "--- What is Replication in Cassandra?\n",
    "Replication in Cassandra ensures data availability and fault tolerance by copying data across multiple nodes. The number of copies (replicas) is determined by the Replication Factor (RF).\n",
    "\n",
    "--- What is SimpleStrategy?\n",
    "SimpleStrategy is the most basic replication strategy in Cassandra. It is used for single-data-center deployments and determines which nodes store the replicas based on the token ring.\n",
    "> How It Works\n",
    "The first replica is placed on the node responsible for the token (i.e., the node where the partition key hashes to).\n",
    "Additional replicas are placed on the next nodes clockwise in the token ring.\n",
    "> Important: SimpleStrategy does NOT consider multiple data centers. If you're using a multi-data-center setup, you should use NetworkTopologyStrategy instead.\n",
    "> Use only for a single datacenter and one rack. SimpleStrategy places the first replica on a node determined by the partitioner. Additional replicas are placed on the next nodes clockwise in the ring without considering topology (rack or datacenter location).\n",
    "\n",
    "--- Two replication strategies are available:\n",
    "> SimpleStrategy: Use only for a single datacenter and one rack. If you ever intend more than one datacenter, use the NetworkTopologyStrategy.\n",
    "> NetworkTopologyStrategy: Highly recommended for most deployments because it is much easier to expand to multiple datacenters when required by future expansion.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment4_Cassandra 2025-02-17 16:16:14",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}